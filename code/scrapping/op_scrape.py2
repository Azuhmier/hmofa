#!/usr/bin/env python3
import requests
from bs4 import BeautifulSoup
import time
import re
import pprint
import numpy as np
import copy

Bad_Threads = [
    "47044936",
]
corrections = {
        "authors" : {
            "kaktus" : [ "kaktus-nsfw" ],
            "namechangedaily" : [ "changed_daily" ],
        },
        "titles" : {
            "Change Your Mind Issue" : ["change your mind"],
            "my waspu" : ["my waspfu"],
        },
        "urls" : {
        },
}

top = 0
def check_anon(author_hash):
    global top
    if re_anon.match(author_hash["author"]) :
        1
    else :
        add_titles(author_hash)
        return 0
    titles = [ author_hash["titles"][idx]["title"] for idx in range(0,len(author_hash["titles"])) ]
    hsh = obj_hash["title"]
    anon_mat = [ hsh[title]["anon"] for title in titles if "anon" in hsh[title].keys() ]
    num = anon_mat[0] if len(anon_mat) != 0  else (top + 1)
    if len(anon_mat) == 0:
        top = top + 1
    for title in titles:
        hsh[title]["anon"] = num

    #[ hsh[title]["anon"] = num for title in titles ]
    add_titles(author_hash)
    return num

def add_titles(author_hash):
    hsh = obj_hash['author']
    name = author_hash['author']
    if not 'titles' in hsh[name].keys():
        hsh[name]['titles'] = [];
    
    for item in author_hash['titles']:
        hsh[name]['titles'].append(item)



#######################
##------ FETCH ----- ##
#######################

pp = pprint.PrettyPrinter(indent=4)
re_author =  re.compile("^\s*[Bb]y\s?\s*(.*)")
re_title  =  re.compile("^>\s*(.+?)\s*([(\[].+[)\]])*$")
re_url    =  re.compile("^(http[^ ]+?)\s*([(\[].+[)\]])*$")
re_end    =  re.compile("^\[")
re_begin  =  re.compile("^\s*-*[sS]tories-*\s*$")
re_anon  =  re.compile("Anonymous|Anon")

url_head = 'https://desuarchive.org/trash/search/text/%22Human%20Males%20On%20Female%20Anthros%20General%22/type/op/page/'
session = requests.Session()
OP_list  = []
obj_hash = {"author":{},"title":{},"url":{} };
for num in range(1,7) :
#for num in [1] :
    url = url_head+str(num)
    r = session.get(url)
    soup = BeautifulSoup(r.content, 'html.parser')
    posts = soup.find_all("article", {"class" : re.compile("post doc_id_")})
    for post in posts :
        hsh = {}
        hsh["thread_num"] = post["id"]
        date = re.compile("4chan Time: (.*)")
        hsh["date"] = date.match(post.find("span", {"class" : "time_wrap"}).time["title"]).group(1)
        div = post.find("div", {"class" : "text"})
        hsh["authors"] = [];
        pointer = hsh["authors"]
        flag = 0
        idx=0
        for line in div.get_text(separator="\n").splitlines() :
            author = re_author.match(line)
            title  = re_title.match(line)
            url    = re_url.match(line)
            begin  = re_begin.match(line)
            if flag :
                if author :
                    if len(hsh["authors"]) != 0:
                        last_author = hsh["authors"][-1]["author"]
                        num = check_anon(hsh["authors"][-1])
                        if num:
                            obj_hash["author"]["Anonymous#" + str(num)] = copy.deepcopy( obj_hash["author"][last_author])
                            obj_hash["author"].pop(last_author,None)

                            hsh["authors"][-1]["author"] ="Anonymous#" + str(num)

                    idx=idx+1
                    if author.group(1) not in obj_hash["author"]:
                        obj_hash["author"][author.group(1)] = {"idx":idx}
                    hsh["authors"].append({ "author" : author.group(1) })
                    hsh["authors"][-1]["titles"] = []
                elif title :
                    if title.group(1) not in obj_hash["title"]:
                        obj_hash["title"][title.group(1)] = {"idx":idx}
                    hsh["authors"][-1]["titles"].append({ "title" : title.group(1) })
                    hsh["authors"][-1]["titles"][-1]["attr"] = title.group(2)
                    hsh["authors"][-1]["titles"][-1]["urls"] = []
                elif url :
                    if url.group(1) not in obj_hash["url"]:
                        obj_hash["url"][url.group(1)] = {"idx":idx}
                    hsh["authors"][-1]["titles"][-1]["urls"].append({ "url" : url.group(1) })
                    hsh["authors"][-1]["titles"][-1]["urls"][-1]["attr"] = url.group(2)
                elif re_end.match(line) :
                    if len(hsh["authors"]) != 0:
                        last_author = hsh["authors"][-1]["author"]
                        num = check_anon(hsh["authors"][-1])
                        if num:
                            obj_hash["author"]["Anonymous#" + str(num)] = copy.deepcopy( obj_hash["author"][last_author])
                            obj_hash["author"].pop(last_author,None)

                            hsh["authors"][-1]["author"] ="Anonymous#" + str(num)

                    break
            if begin :
                flag = 1
        OP_list.append(hsh)
    time.sleep(0.5)

author_stack = []
for op in OP_list[::-1]:
    #print(op["date"])
    for item in op["authors"][::-1]:
        author_stack.insert(0,(item["author"]))
        #if re_anon.match(item["author"],re.IGNORECASE):
        for idx, author in enumerate(author_stack[1:]):
            if item["author"].lower() == author.lower() :
                author_stack.pop(idx+1)

for author in author_stack:
    print("By " + author)
#new
pp.pprint(obj_hash)
#index change
