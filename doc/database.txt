----------------------------------------------------------------------
Objective
----------------------------------------------------------------------
GPG
- urls --> web scrapping


database
- urls --> web scrapping
- data <-- other masterbins
- data <--> masterbin
- data <-- external data structures
- query
- validation
- story archive
- delta files
- web scrapping
  OP list
  threads
  stories




----------------------------------------------------------------------
Keywods
----------------------------------------------------------------------
array
hash
json
linewise
reference
regex
obj
attribute
file based database
capture groups
substitution
order
point
tree
object value
object type
explicit attribute: attribute and object capture are seperate
implict attribute:  attribute and object capture group are not seperate
false attributes
false objects

----------------------------------------------------------------------
Data Structure
----------------------------------------------------------------------
obj_hash
{
    value       : "object value"
    type        : "object type"
    attributes  : {...}
    childs      : {...}
    meta        : {...}
}

childs_hash
{
   child-1 : [childs-1 array]
   child-2 : [childs-2 array]
    ...
   child-i : [childs-i array]
    ...
   child-n : [childs-n array]
}

childs-i array
[
    {obj-i hash-1}
    {obj-i hash-2}
    ...
    {obj-i hash-j}
    ...
    {obj-i hash-n}
]

childs_stack
[
    {obj-[1:n] hash-1}
    {obj-[1:n] hash-2}
    ...
    {obj-[1:n] hash-j}
    ...
    {obj-[1Ln] hash-n}
]

circs
{
    obj hash ref key :
    {
        parent      : parent hash ref key
        child stack : [...]
    }
}

* circs can be recreated by transversal of hash tree.
* a objhash without a LN, can be a fatal error.

----------------------------------------------------------------------
Deimos
----------------------------------------------------------------------
0.0) author
1.0) title
2.0) url

By Author
>title
url

By {Author}
>{title}
{url}


Rules
- 

----------------------------------------------------------------------
Deimos_exstended
----------------------------------------------------------------------
0.0) section
1.0) author
2.0) series
3.0) title
4.0) tags
4.1) url
4.2) description

{object} = {object_value}
%%%%% {section} %%%%%
By {author}
=============/ {series} /=============
">{title}"
"[{tags}]"
"{url}"
"#{description}"

----------------------------------------------------------------------
Deimos_exstended (implicit attributes}
----------------------------------------------------------------------
0.0)   section
1.0)   author
1.0.0) author_attribute
2.0)   series
3.0)   title
3.0.0) title_attribute
4.0)   tags
4.1)   url
4.1.0) url_attribute
4.3)   description

{object} = {object_value} + {object_attribute}
{object} - {object_attribute} = {object_value}
object value is the result of subtracting the attributes
{object_value} = "" if attribute is implicit

methods of attribute subtraction
1) capture groups
    EX: [anthro][general]ops
        anthro  = [(.*?)][  | [{anthro}][general]ops
        general = ][(.*?)]  | [anthro][{general}]ops
        ops     = ](.*?)$   | [anthro][general]{ops}
        *subract substrings | ""

2) substitution: assumes attributes are ordered
    EX: [anthro][general]ops
        anthro  = "[.*?]  | "[general]ops"
        general = "[.*?]" | "ops"
        ops     = ".*"    | ""

*Fails upon "" object value


"%%%%% {section} %%%%%"
"By {author}"
"=============/ {series} /============="
">{title}"
"{tags}"
"{url}"
"#{description}"

"%%%%% {section_value} %%%%%"
"By {author_value} ({author_attribute})"
"=============/ {series_value} /============="
">{title_value} ({title_attribute})"
"{tags_value}"
"{url_value} ({url attribute})"
"#{description_value}"

----------------------------------------------------------------------
Deimos_exstended (implicit attributes and containers}
----------------------------------------------------------------------
0.0)   section
1.0)   author
1.0.0) author_attribute
2.0)   series
3.0)   title
3.0.0) title_attribute
(
    4.0)      tags_container
    4.0.0)    anthro
    4.0.0.0)  tag_attribute
    4.0.1)    general
    4.0.1.0)  tag_attribute
    4.0.2)    ops
)
4.1)   url
4.1.0) url_attribute
4.2)   description

"%%%%% {section} %%%%%"
"By {author}"
"=============/ {series_value} /============="
">{title}"
"{tags}"
"{url}"
"#{description}"

"%%%%% {section_value} %%%%%"
"By {author_value} ({author_attribute})"
"=============/ {series_value} /============="
">{title_value} ({title_attribute})"
"{tags_value}" => "[{tags_attribute1}][{tags_attribute2}]{tags_attribute3}"
"{url_value} ({url attribute})"
"#{description_value}"

*"{tags_value}[{tags_attribute1}][{tags_attribute2}]{tags_attribute3}"
*containers must resolve to "" value
*obj vs. linecontainer

----------------------------------------------------------------------
Deimos_exstended (delimited implicit attributes and objects}
----------------------------------------------------------------------
changes the attribute value from scalar to array
*conserv order?
*conserve delimiter?

0.0)     section
1.0)     author
1.0.0)   author_attribute
2.0)     series
3.0)     title-url container
(
    3.0.0)   title
    3.0.0.0) title_attribute
    3.0.1)   url
    3.0.1.0) url_attribute
--------------------------------
    3.0.0)   title
    3.0.1)   url
    3.0.1.0) url_attribute
    3.0.2)   title_attribute
)
4.0)     tags
4.1)     description

limitaions
- attributs must come after the object value
- attributes must be on the same line as the attributes
- keys can only have the following characters [a-zA-Z_0-9]
- all childs are grouped with eachother

The general idea of this program (OHMFA) is to create a dynamic program that
takes user inputs, configurations, and execution history and parses a masterbin
file, produces a flat file (Ex: json) and uses this file to create a more
sophisticated database (DB), for example SQL. These concepts are illustrated in
Figure 1 below.

While most programs/projects might just see the flat file generation as a mere
intermediate step to the primary goal of a database, OHMFA creates additional
files and stores them along with the flat file in a directory called '.ohmfi'
that serves as a local 'database' for the masterbin file and it's contents -
kinda sorta like git, though no implementations of actual delta files has been
done.

As of right now, this 'local database' is the one being used to check masterbin
changes, edit tags, query, and output rentry txt that is uploaded to
rentry.org. There are 3 main 'methods' it has

1) init
Take a masterbin file and attempt to create an '.ohmfi' directory out of it,
success is determined when it can write back the entire file exactly and parse
it again and get the same flat file. Every time it runs it will show file
and flat filediffs using the less program, thus, it mean to be used to quickly
check ones syntaxing and go and make quick edits. Parsing validationg is
'syntax' like because the user will be editing the flat file by editing the
masterbin.

2) manipulate
Like above except success is mainly determined by user, by looking at the
filediffs betwwen the previous and the newly edited masterbin file and flat
file. Theroretically, conditions can be put up to check for fundemental
mistakes such as hierarchical inconsistency and duplicates.

3) compare (this has not been implemented yet)
Be able to parse an external masterbin, compare it's content, and merge any
wanted data. The parsing validation best described as 'text wrangling', since
only parts of the file may be relevant as data.

Even though much of the previous concepts have been implemented, many were done
poorly or partially. This is the first time I had to describe the project in
great technical detail for someone else to understand, the theme of this whole
project so far has been 'jerry rigging' to get to the next wanted feature or
objective. This makes it harder to improve, expand, or update it. I am having
to start from square one and build a strong fondation, and first thing I need
to do is to establish a storg 'data architecture'. My current attempt can be
seen in Table 1 below.

Note that 'a' stands for attributes, 'v' stands for value or object value (I
call my 'data fields' objects), and a '@' signifies and array of either of the
two properties. All regex expression you currently see are the ones being
implemented. So the obstacle I am trying to find or test my solution against is
how to store the data in a way that will make it easily transferable to a
database as well as still be of practical use for local masterbin file
operations.

There are MANY more concepts implemented that I have not gone over, and I made
this email in a rush for I did not notify you before hand it would take this
long. For the sake of your time, here is the TL;DR version of the prior excerpt
along with additional questions.

TL;DR




I apologize for taking long to reply to you, work has been busy and
unpredictable.  The general idea of this program (OHMFA) is to create a dynamic
program that takes user inputs, configurations, and execution history and
parses a masterbin file, produces a flat file (Ex: json) and uses this file to
create a more sophisticated database (DB), for example SQL. These concepts are
illustrated in Figure 1 below

I have many questions but I will start off this first email with my most
pressing one. My only understanding of databases comes from a brief look at SQL
as well as plain hash tables, where everything is a table. My problem is trying
to repereent field inputs that are arrays, especially in the context of
heiarchal data such as this one. For example how should one repersent tags in a
data base? When one looks up the 'tags' key for a story?  compose a myriad ways
of repersenting array objects, but I have no idea if they are practical. The
following is the current index syntax of my current implentation of a
masterbin flat-file can be seen at:

root['childs']['section'][0]['childs']['author'][4]['childs']['title'][2]['childs']['tags'][0]['val'][3] = 'canine'
root['childs']['section'][0]['childs']['author'][4]['childs']['title'][2]['childs']['tags'][0]['attrs']['anthro'][3] = 'canine'

I just shortened it because it is easier to read it the other way, and that is
the way I use when brainstorming. As you can see this is a bit nonsensical with
a value and an attribute having the same value, thus I have been experimenting
with doing following:

a)
root['childs']['section'][0]['childs']['author'][4]['childs']['title'][2]['childs']['tags'][0]['childs']['anthro'][0]['childs']['tag'][3]['val'] = 'canine'
b)
root['childs']['section'][0]['childs']['author'][4]['childs']['title'][2]['childs']['tags'][0]['val'][3] = 'canine'
root['childs']['section'][0]['childs']['author'][4]['childs']['title'][2]['childs']['tags'][0]['attrs'][0] = 'anthro'
c)
root['childs']['section'][0]['childs']['author'][4]['childs']['title'][2]['childs']['tags'][0]['attrs']['tag_group'] = 'anthro'

For further insight into the flat-file architecture see figure 2: below. It is
similar to the current one in use besides there being a ‘props’ and ‘ID’ key in
the Object hash. These object hash keys are currently being conceptualize for
their use.


My only experience with the C language is through my brush with embedded
programing, and besides that I have barley even gotten into C/C++. That said,
C/C++ is definitely something I want to eventually integrate or include in this
project, primarily to do all the heavy lifting with text parsing, diffing
alogrotythms, file deltas, and flat-file queuing and manipulation. My current
program written in perl takes about 400 ms just to make a flat-file and rewrite
the input file and compute diffs of the input file and the flat-file with the
previous ones. In order to perform more complex operations and parsing I have
no choice but to look into C/C++ or other compiled languages. The masterbin is
not small about with over 6000+ lines. The sceenshot below sumarizes the amount
of items for each field in the 'database'

That is a total of 10,905 items, each with their own hash and attributes that
need to be parsed, categorised, diff'd and written back. My use of an
interpeted language as put the execution time at about 400 - 2000ms. It
prevents me from improving or expanding the program due to exponential
increases in execution time to the point where the program is unusable,
needless to say this bogs down any potential testing of more rigourous
operations and makes causes me to have aversion to recursing through the data
structuere again, even though such operations would only make the program
better.

I am not 'comfortable' (the best word I could think of at the moment) with
telling you what you can do because this is basically a hobby of mine and I can
garuntee no long term deadlines do and will have sometimes a weeks or two of no
activity on the project due to my work schedule and current job searching. Also
I am still trying to garner a clear technical objective for this program beyond
flowcharts, so I can concisly illustrate objectives, I am currently working on
that and othe documentations. THe figures and tables included were not even the
tip of the iceberg. However, if you want a genral notion of what you can do it
would be to develop operations of text parsing a masterbin file into a flat
file and doing operations of said file in C++, but that is such a genral and
broad objective that its basically stating to do the project for me lol. I
guess to try to narrow it down, my main concern would be the text parser and the
alogrythm used in C++. I just want to be able to integrate a C++ program into
an interpeted language that can allow me to tranverse the flatfile and parsing
proess much more rigourously. I can't give you any technically concise
objectives besides the ones listed in the prior table and I an not going to ask
something to much or too broad with a person I am just now being aquainted
with. I am basically trying to rescruture the program at this point and if it
requires me to move from perl to python or any other language, I would be more
than fine with that. So yeah, in summary, dynamic/complex file parsing in
relation to the prior table and flat-file operations for all 10,000+ items. I'm
apoligize if you dissapointed in not having a much more specfic objective in
what you can do, part of it is my limited knowledge of the C++ programing
itself, I imagine I would have to learn a bit more to get an idea of what you
CAN do with it.

This is an ongoing project that I am currently trying to document and organize
at the moment to allow more people to contribute, so a day, week, or even a
month from now I could possibly have a more specfic objective in terms of C++.
If you want, I can keep you updated on the progress of the project as well with
newly composed technical documentation and objectives. C/C++ integration into
this project is inevetible and I'm going to make it happen as to when I am not
sure. This little project it the culminationnn of nearly 4 years of working on
and off on my hobby to categorize the /hmofa/ masterbin and make that data available to
others.

Thank you so much for your time, and I look forward to hearing back from you.

My only experience with the C language is through my brush with embedded
programing, and besides that I have barley even gotten into C/C++. That said,
C/C++ is definitely something I want to eventually integrate or include in this
project, primarily to do all the heavy lifting with text parsing, diffing
algorithms, file deltas, and flat-file queuing and manipulation. My current
program written in Perl takes about 400 ms just to make a flat-file and rewrite
the input file and compute diffs of the input file and the flat-file with the
previous ones. In order to perform more complex operations and parsing I have
no choice but to look into C/C++ or other compiled languages. The masterbin is
not small about with over 6000+ lines. The screenshot below summarizes the
amount of items for each field in the 'database'

That is a total of 10,905 items, each with their own hash and attributes that
need to be parsed, categorized, diff'd and written back. My use of an
interpreted language as put the execution time at about 400 - 2000ms. It
prevents me from improving or expanding the program due to exponential
increases in execution time to the point where the program is unusable,
needless to say, this bogs down any potential testing of more rigorous
operations and makes causes me to have aversion to recursing through the data
structure again, even though such operations would only make the program
better.

I am not 'comfortable' (the best word I could think of at the moment) with
telling you what you can do because this is basically a hobby of mine and I can
guarantee no long-term deadlines do and will have sometimes a week or two of no
activity on the project due to my work schedule and current job searching. Also
I am still trying to garner a clear technical objective for this program beyond
flowcharts, so I can concisely illustrate objectives, I am currently working on
that and other documentations. The figures and tables included were not even
the tip of the iceberg. However, if you want a general notion of what you can
do it would be to develop operations of text parsing a masterbin file into a
flat file and doing operations of said file in C++, but that is such a general
and broad objective that its basically stating to do the project for me lol. I
guess to try to narrow it down, my main concern would be the text parser and
the algorithm used in C++. I just want to be able to integrate a C++ program
into an interpreted language that can allow me to transverse the flat file and
parsing proses much more rigorously. I can't give you any technically concise
objectives besides the ones listed in the prior table and I don’t want to ask
something to much or too broad with a person I am just now being acquainted
with. I am basically trying to restructure the program at this point and if it
requires me to move from Perl to python or any other language, I would be more
than fine with that. So yeah, in summary, dynamic/complex file parsing in
relation to the prior table and flat-file operations for all 10,000+ items. I'm
apologize if you disappointed in not having a much more specific objective in
what you can do, part of it is my limited knowledge of the C++ programing
itself, I imagine I would have to learn a bit more to get an idea of what you
CAN do with it.

This is an ongoing project that I am currently trying to document and organize
at the moment to allow more people to contribute, so a day, week, or even a
month from now I could possibly have a more specific objective in terms of C++.
If you want, I can keep you updated on the progress of the project as well with
newly composed technical documentation and objectives. C/C++ integration into
this project is inevitable and I'm going to make it happen as to when I am not
sure. This little project it the culmination of nearly 4 years of working on
and off on my hobby to categorize the /hmofa/ masterbin and make that data
available to others.

Thank you so much for your time, and I look forward to hearing back from you.


